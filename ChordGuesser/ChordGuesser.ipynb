{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-26T18:02:29.588994700Z",
     "start_time": "2024-04-26T18:02:29.577993500Z"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.feature\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def plot_song(data):\n",
    "    plt.plot(data)\n",
    "    plt.xlim([0, len(data)])\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T18:02:29.844963200Z",
     "start_time": "2024-04-26T18:02:29.841966100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def import_song(path_to_song):\n",
    "    data, samplerate = librosa.load(path_to_song)\n",
    "    return data, samplerate\n",
    "\n",
    "def show_song(path_to_song):\n",
    "    data, samplerate = import_song(path_to_song)\n",
    "    plot_song(data)\n",
    "\n",
    "def get_song_tempo(path_to_song):\n",
    "    data, samplerate = import_song(path_to_song)\n",
    "    song_onset = librosa.onset.onset_strength(y=data, sr=samplerate)\n",
    "    tempo = librosa.feature.tempo(onset_envelope=song_onset, sr=samplerate)\n",
    "    return tempo\n",
    "\n",
    "def get_song_pitches(path_to_song):\n",
    "    data, samplerate = import_song(path_to_song)\n",
    "    pitches, magnitudes = librosa.piptrack(y=data, sr=samplerate)\n",
    "    return pitches, magnitudes\n",
    "\n",
    "def get_max_pitches(pitches, magnitudes, number_of_notes, time=30):\n",
    "    initial_chord = magnitudes[:, time]\n",
    "    max_index = np.argpartition(initial_chord, -number_of_notes)[-number_of_notes:]\n",
    "    max_pitches = pitches[max_index, time]\n",
    "    scale_to_fourth_octave(max_pitches)\n",
    "    return np.sort(max_pitches)\n",
    "\n",
    "'''\n",
    "All I care about is predicting the chord accurately, not the octave information\n",
    "I scale each note to be in the 4th octave (440 - 800 Hz), this allows:\n",
    "1) More samples\n",
    "2) More accurate predictions using KNN since each chord is now 1 cluster as opposed to 1 cluster per octave.\n",
    "For instance, without this, if we get an A6 chord it's nearest neighbor would be G5,\n",
    "it would not be able to use A3, A4, A5, etc information.\n",
    "3) Overtones get scaled down as well\n",
    "\n",
    "For example, a C triad which is C (522 Hz) E (660 Hz) G (784 Hz) may have overtones that are factors of these.\n",
    "I.E. the three dominant pitches could be 522 Hz, 784 Hz, and 1044 Hz. This will at least allow me to scale the\n",
    "over tones down to the proper range\n",
    "'''\n",
    "\n",
    "def scale_to_fourth_octave(pitches):\n",
    "    scale_low_notes_up(pitches)\n",
    "    scale_high_notes_down(pitches)\n",
    "\n",
    "def scale_low_notes_up(pitches):\n",
    "    if 0 not in pitches:\n",
    "        scale_up_factor = np.floor(np.log2(440 / pitches))\n",
    "        scale_up_factor[scale_up_factor < 0] = 0\n",
    "        pitches *= 2 ** scale_up_factor\n",
    "\n",
    "def scale_high_notes_down(pitches):\n",
    "    if 0 not in pitches:\n",
    "        scale_down_factor = np.ceil(np.log2(pitches / 880))\n",
    "        scale_down_factor[scale_down_factor < 0] = 0\n",
    "        pitches /= 2 ** scale_down_factor\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T22:19:40.253973Z",
     "start_time": "2024-04-26T22:19:40.235977400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "NOTE_COUNT = 4\n",
    "PATH_TO_AUDIO_DATA = \"C://Users//Arthur//Desktop//audio_augmented_x10\"\n",
    "\n",
    "'''\n",
    "Data source: https://zenodo.org/records/5217057\n",
    "\n",
    "File data has the naming convention:\n",
    "- 3 octaves (3, 4, 5).\n",
    "- 12 base notes per octave: Cn, Df, Dn, Ef, En, Fn, Gf, Gn, Af, An, Bf, Bn. (n is natural, f is flat).\n",
    "- 4 triad types per note: major (j), minor (n), diminished (d), augmented (a). No inversions.\n",
    "- 3 volumes per triad: forte (f), metsoforte (m), piano (p).\n",
    "- Metadata is in the name of the chord. For example: \"piano_4_Af_d_m_45.wav\" is a piano chord, (4) 4th octave,\n",
    "(Af) A flat base note, (d) diminished, (m) metsoforte, 45th example.\n",
    "'''\n",
    "\n",
    "MAJOR_FILE_NAME_SHORTHAND = \"_j_\"\n",
    "MINOR_FILE_NAME_SHORTHAND = \"_n_\"\n",
    "\n",
    "def initialize_model():\n",
    "    training_labels, training_pitches, validation_labels, validation_pitches = split_training_validation_data()\n",
    "\n",
    "    model = neighbors.KNeighborsClassifier(n_neighbors=4)\n",
    "    model.fit(training_pitches, training_labels)\n",
    "    validate_model(model, validation_labels, validation_pitches)\n",
    "    return model\n",
    "\n",
    "def split_training_validation_data():\n",
    "    training_labels, training_pitches, validation_labels, validation_pitches = [], [], [], []\n",
    "\n",
    "    for file in os.listdir(PATH_TO_AUDIO_DATA):\n",
    "        # Focus on major/minor for now - soon will remove this and train on entire dataset (diminished, augmented)\n",
    "        if chord_is_major_or_minor(file):\n",
    "            if file_is_training(file):\n",
    "                append_data(training_labels, training_pitches, file)\n",
    "            else:\n",
    "                append_data(validation_labels, validation_pitches, file)\n",
    "\n",
    "    return training_labels, training_pitches, validation_labels, validation_pitches\n",
    "\n",
    "def chord_is_major_or_minor(file_name):\n",
    "    return MAJOR_FILE_NAME_SHORTHAND or MINOR_FILE_NAME_SHORTHAND in file_name\n",
    "\n",
    "# Want to save some data for verification.\n",
    "# 100 samples per chord type, so we train on the first 80 and save the last 20 for validation.\n",
    "def file_is_training(file_name):\n",
    "    return get_file_number(file_name) <= 80\n",
    "\n",
    "def get_file_number(file_name):\n",
    "    return int(file_name[-6:-4])\n",
    "\n",
    "def append_data(labels, pitches_data, file):\n",
    "    chord = get_chord_name(file)\n",
    "    pitches, magnitudes = get_song_pitches(f\"{PATH_TO_AUDIO_DATA}//{file}\")\n",
    "    max_pitches = get_max_pitches(pitches, magnitudes, NOTE_COUNT)\n",
    "    pitches_data.append(max_pitches)\n",
    "    labels.append(chord)\n",
    "\n",
    "def get_chord_name(file_name):\n",
    "    chord_root = file_name[8:10]\n",
    "    chord_type = assign_chord_type(file_name)\n",
    "    chord_name = f\"{chord_root} {chord_type}\"\n",
    "    return chord_name\n",
    "\n",
    "def assign_chord_type(file_name):\n",
    "    if MAJOR_FILE_NAME_SHORTHAND in file_name:\n",
    "        chord_type = \"Major\"\n",
    "    else:\n",
    "        chord_type = \"Minor\"\n",
    "    return chord_type\n",
    "\n",
    "def validate_model(model, validation_labels, validation_pitches):\n",
    "    predictions = model.predict(validation_pitches)\n",
    "    print(f\"accuracy is {accuracy_score(validation_labels, predictions)}\")\n",
    "    print(f\"F1 is {f1_score(validation_labels, predictions, average='micro')}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T22:20:09.204967200Z",
     "start_time": "2024-04-26T22:20:09.199966700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.9885477582846004\n",
      "F1 is 0.9885477582846004\n"
     ]
    }
   ],
   "source": [
    "trained_model = initialize_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T22:34:16.770706200Z",
     "start_time": "2024-04-26T22:20:09.489993500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Af Major', 'En Minor', 'Gn Major']\n",
      "['An Minor', 'Cn Major', 'En Minor']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def get_prediction(pitches, magnitudes, time_stamp):\n",
    "    max_pitches = get_max_pitches(pitches, magnitudes, NOTE_COUNT, time_stamp)\n",
    "    chord_prediction = trained_model.predict([max_pitches])\n",
    "    return chord_prediction[0]\n",
    "\n",
    "def test_song(song_path):\n",
    "    pitches, magnitudes = get_song_pitches(song_path)\n",
    "    song_length = len(pitches[0])\n",
    "\n",
    "    song_tempo_int = song_length // 3 # Need a more sophisticated way of finding chord changes\n",
    "                                      # good enough while I figure out the 3 chord progression samples\n",
    "    predictions = []\n",
    "\n",
    "    for time_stamp in range(0, song_length - 3, song_tempo_int):\n",
    "        prediction = get_prediction(pitches, magnitudes, time_stamp)\n",
    "        predictions.append(prediction)\n",
    "    print(predictions)\n",
    "\n",
    "# C major, E minor, G major on each downbeat (3 beats), 120 bpm\n",
    "# Predicts the C triad as A flat\n",
    "# A flat contains similar notes so needs some refinement\n",
    "test_song(\"Songs/C_Em_G_progression.wav\")\n",
    "\n",
    "# A minor, C major, E minor on each downbeat (3 beats), 120 bpm\n",
    "test_song(\"Songs/Am_C_Em_progression.wav\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T22:34:16.881990Z",
     "start_time": "2024-04-26T22:34:16.778708700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
